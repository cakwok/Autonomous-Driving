The main objective of this project is to develop a fusion system that combines camera and LiDAR to track various objects, including vehicles and pedestrians, in urban environments. 

## Tracking
We initialize a new track with the first measurement.  Each time when we receive new measurements from one of the sensors,  the update function is triggered.  If the measurement is generated by a LiDAR sensor, we can simply apply a standard Kalman filter to update the vehicle state.  However, camera measurements involves a non-linear measurement function and we will use an Extended Kalman Filter to handle this non-linearity. 

In this task, a 6D F matrix to used predict positions and velocities in xyz dimensions, and a Q matrix to estimate both noise covariance.  Then at the update step, the residual values for camera sensor is computed by a linear approximation, first order Taylor expansion, such that after image coordinates transformation, the resulted would still in a Gaussian distribution.

<img src="https://user-images.githubusercontent.com/21034990/224194307-fc91fc2b-f912-4cf7-ace7-0598c21ce95b.png" width=500><img src="https://user-images.githubusercontent.com/21034990/224194319-0115dbda-dab3-4eb8-a487-05a1a322700a.png" width=500>

## Track Management
In real world scenario, we usually have multiple objects in the vehicles environment and we also receive multiple measurements at each time stamp.

Track management is about initalization of new tracks, deletion of old tracks, and assignment of confidence value to a track.

In this task, I have initialized a track by transforming a measurement from sensor to vehicle coorinates.  Then keep track of unassigned tracks over a window size. 

If a track is in confirmed state but has fallen below a threshold, the track would be deleted.  Initialized and tentative tracks, on the other hand, since we don't have enough track score to reference, we can refer to their noise covariance instead.  Shall the uncertainty exceed a certain values, the track could be be discarded.

Otherwise, a confirmed track would be increased in track score so we would have a confidence level.

<img width="500" alt="image" src="https://user-images.githubusercontent.com/21034990/224194171-a4891c20-ff05-4e7b-862e-53c316ba9d80.png"><img width="500" alt="image" src="https://user-images.githubusercontent.com/21034990/224194042-d1b076ef-21ac-474b-a6e3-7c8133ebbb98.png">

## Data Association
We have established tracks after the prior step.  In this step, we calculated the Mahalanobis distances between each pair of tracks and measurement, take the single nearest neighbor data to associate measurements to tracks, and used a gating technique with chi-square-distribution to remove outliners.
<img width="1245" alt="image" src="https://user-images.githubusercontent.com/21034990/224204659-7fae6e5b-d2ed-4f4f-a085-ab1eadb3ba67.png"> 
<img width="1238" alt="image" src="https://user-images.githubusercontent.com/21034990/224203648-a8f81344-a9c0-45a0-9897-a44fc99a4f36.png">

## Sensor Fusion
To ensure accurate track management in autonomous vehicles, it is essential to consider the visibility of an object across multiple sensors. Therefore, we have implemented a method that checks if an object falls within a sensor's field of view before executing track management. This ensures that only valid objects are considered for further processing, while objects outside of the sensors' range are ignored, and their track score remains unchanged.
<img width="1267" alt="image" src="https://user-images.githubusercontent.com/21034990/224209913-a021082c-4f3c-426b-bd82-919e6eebcd16.png">
<img width="1264" alt="image" src="https://user-images.githubusercontent.com/21034990/224209865-08542e88-9e68-4498-bfa3-d57359c1240a.png">
<a href="https://youtu.be/uiIsP0S6V5Y">Link to video</a>

## Summary
- The most difficult part for me to complete is the general workflow to understand the holistic meaning of the pipelines and also to understand the given codes.  Otherwise I think the teaching materials are clear to explain each individual concept.
- Yes I have seen improvement of the MSE plotting over time between part 2 LiDAR only and the last part with fusion.  The mean of MSE has dropped from 0.78 to 0.18.  It works as my expectation, as more dataset provided, the higher accuracy should a model to be achieved.
- A sensor fusion system is designed to integrate data from multiple sensors to improve the accuracy and reliability of measurements and help in making more informed decisions. However, there are several challenges that sensor fusion systems can face in real-life scenarios, sensor diversity, sensor calibration, fusion algorithms and environmental conditions.  The challenge I have seen in this project is about higher computation requirement and complexity of fusion algorithms.
- To improve tracking results in the future, possible strategies could be incorportion of multi modal informations, such as weather conditions and map data.
